#!/usr/bin/env python3
"""
Test Query Processing & Response Generation - Step 4 of RAG Pipeline

This script demonstrates the query processing capabilities:
- Intelligent query understanding and expansion
- Multi-modal evidence retrieval from vector database
- LLM-powered response generation with citations
- Interactive question-answering interface
- Response quality assessment and follow-up suggestions
"""

import os
import sys
from pathlib import Path

# Add the core_pipeline directory to Python path
sys.path.append(str(Path(__file__).parent.parent))
# Load environment variables
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("âœ… Loaded environment variables from .env file")
except ImportError:
    print("âš ï¸  python-dotenv not installed, using system environment variables only")

from core_pipeline.query_processor import (
    QueryProcessor,
    ask_question,
    interactive_query_session
)


def test_query_processor():
    """Test the complete query processing and response generation workflow"""
    
    print("ğŸ¤– Query Processing & Response Generation Test - Step 4")
    print("=" * 60)
    
    # Check required environment variables
    firebase_key = os.getenv("FIREBASE_SERVICE_KEY")
    openai_key = os.getenv("OPENAI_API_KEY")
    
    if not firebase_key:
        print("âŒ FIREBASE_SERVICE_KEY environment variable not set")
        print("ğŸ’¡ This test requires Firebase integration")
        return False
    
    if not openai_key:
        print("âš ï¸  OPENAI_API_KEY not set - will use fallback responses")
    
    try:
        # Step 1: Initialize Query Processor
        print("ğŸ—ï¸  Step 1: Initializing Query Processor...")
        processor = QueryProcessor(
            enable_openai=bool(openai_key),
            max_context_length=8000,
            response_max_tokens=1000
        )
        print("âœ… Query processor initialized successfully")
        print(f"   â€¢ OpenAI enabled: {processor.enable_openai}")
        print(f"   â€¢ Firebase enabled: {processor.enable_firebase}")
        print(f"   â€¢ Max context length: {processor.max_context_length}")
        
        # Step 2: Get available vector databases
        print(f"\nğŸ“‹ Step 2: Finding available vector databases...")
        available_dbs = processor.embedder.list_vector_databases()
        
        if not available_dbs:
            print("âŒ No vector databases found")
            print("ğŸ’¡ Please run vector_embedder.py first to create a vector database")
            return False
        
        latest_db = available_dbs[0]
        db_id = latest_db['database_id']
        
        print(f"âœ… Found {len(available_dbs)} vector databases")
        print(f"   â€¢ Using latest: {db_id}")
        print(f"   â€¢ Total chunks: {latest_db['total_chunks']}")
        print(f"   â€¢ Documents: {latest_db['documents_count']}")
        print(f"   â€¢ Model: {latest_db['embedding_model']}")
        
        # Step 3: Test Query Understanding
        print(f"\nğŸ§  Step 3: Testing query understanding and analysis...")
        test_queries = [
            "What are the main health outcomes discussed in the research?",
            "How was the study methodology designed?",
            "What statistical findings support the conclusions?",
            "Compare the results between different groups",
            "Summarize the key research findings"
        ]
        
        for i, query in enumerate(test_queries):
            print(f"\n   Query {i+1}: '{query}'")
            query_context = processor._analyze_query(query)
            print(f"      Type: {query_context.query_type.value}")
            print(f"      Confidence: {query_context.intent_confidence:.2f}")
            print(f"      Key concepts: {', '.join(query_context.key_concepts[:3])}")
            print(f"      Requires tables: {query_context.requires_tables}")
            print(f"      Requires stats: {query_context.requires_statistics}")
        
        # Step 4: Test Evidence Retrieval
        print(f"\nğŸ” Step 4: Testing evidence retrieval...")
        test_query = "What are the health outcomes for children in the study?"
        print(f"   Query: '{test_query}'")
        
        query_context = processor._analyze_query(test_query)
        evidence = processor._retrieve_evidence(query_context, db_id, max_results=5, use_hybrid=True)
        
        if evidence:
            print(f"   âœ… Retrieved {len(evidence)} evidence pieces:")
            for i, item in enumerate(evidence[:3]):
                print(f"      {i+1}. Relevance: {item.relevance_score:.3f}")
                print(f"         Section: {item.section}")
                print(f"         Type: {item.content_type}")
                print(f"         Content: {item.content[:80]}...")
        else:
            print("   âŒ No evidence retrieved")
        
        # Step 5: Test Complete Query Processing
        print(f"\nğŸ’¬ Step 5: Testing complete query processing...")
        comprehensive_queries = [
            "What were the main findings about childhood development?",
            "How did the researchers measure health outcomes?",
            "What statistical methods were used in the analysis?"
        ]
        
        for i, query in enumerate(comprehensive_queries):
            print(f"\n   Processing query {i+1}: '{query}'")
            result = processor.process_query(query, db_id, max_results=5)
            
            print(f"   âœ… Response generated:")
            print(f"      Response type: {result.response_type}")
            print(f"      Confidence: {result.confidence_score:.2f}")
            print(f"      Quality: {result.quality_assessment.value}")
            print(f"      Evidence pieces: {len(result.evidence)}")
            print(f"      Citations: {len(result.citations)}")
            print(f"      Processing time: {result.processing_stats.get('processing_time_seconds', 0):.2f}s")
            print(f"      Response preview: {result.response[:100]}...")
            
            if result.suggested_followups:
                print(f"      Follow-ups: {', '.join(result.suggested_followups[:2])}")
        
        # Step 6: Test Convenience Functions
        print(f"\nğŸ› ï¸  Step 6: Testing convenience functions...")
        
        print("   Testing ask_question() convenience function...")
        convenience_result = ask_question(
            "What are the key research findings?", 
            vector_db_id=db_id,
            use_hybrid=True,
            max_results=3
        )
        
        if convenience_result:
            print(f"   âœ… Convenience function works:")
            print(f"      Confidence: {convenience_result.confidence_score:.2f}")
            print(f"      Response length: {len(convenience_result.response)} chars")
            print(f"      Evidence count: {len(convenience_result.evidence)}")
        
        # Step 7: Interactive Session Demo
        print(f"\nğŸ® Step 7: Interactive session demo...")
        print("   Note: Interactive session available via interactive_query_session()")
        print(f"   To start: python -c \"from query_processor import interactive_query_session; interactive_query_session('{db_id}')\"")
        
        # Step 8: Performance Summary
        print(f"\nğŸ“ˆ Step 8: Query Processing Performance Summary")
        print("-" * 50)
        print(f"   â€¢ Vector Database: {db_id}")
        print(f"   â€¢ Available Documents: {latest_db['documents_count']}")
        print(f"   â€¢ Searchable Chunks: {latest_db['total_chunks']:,}")
        print(f"   â€¢ Query Understanding: Pattern-based + keyword extraction")
        print(f"   â€¢ Response Generation: {'OpenAI gpt-4.1-mini' if processor.enable_openai else 'Template-based fallback'}")
        print(f"   â€¢ Evidence Retrieval: Hybrid semantic + keyword search")
        print(f"   â€¢ Citation Support: âœ… (Automatic source attribution)")
        print(f"   â€¢ Quality Assessment: âœ… (Heuristic-based scoring)")
        print(f"   â€¢ Follow-up Suggestions: âœ… (Context-aware recommendations)")
        
        print(f"\nğŸ‰ Query Processing Test Completed Successfully!")
        print(f"ğŸ¤– Your RAG system can now:")
        print(f"   - Understand user questions intelligently")
        print(f"   - Retrieve relevant evidence from research documents")
        print(f"   - Generate comprehensive answers with citations")
        print(f"   - Assess response quality and confidence")
        print(f"   - Suggest relevant follow-up questions")
        print(f"   - Support interactive Q&A sessions")
        
        return True
        
    except Exception as e:
        print(f"âŒ Query processing test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def demo_question_answering():
    """Demonstrate question answering with sample queries"""
    
    print("\nğŸ¯ Question Answering Demo")
    print("=" * 40)
    
    try:
        # Sample questions to demonstrate different capabilities
        demo_questions = [
            {
                "question": "What are the main health outcomes discussed?",
                "description": "Factual question about research findings"
            },
            {
                "question": "How was the research methodology designed?", 
                "description": "Methodology-focused question"
            },
            {
                "question": "What statistical evidence supports the conclusions?",
                "description": "Statistical analysis question"
            },
            {
                "question": "Summarize the key findings about childhood development",
                "description": "Summarization request"
            }
        ]
        
        print(f"ğŸ” Demonstrating {len(demo_questions)} sample questions...")
        
        for i, item in enumerate(demo_questions):
            print(f"\nğŸ“ Question {i+1}: {item['description']}")
            print(f"   Query: \"{item['question']}\"")
            
            try:
                result = ask_question(item['question'], max_results=3)
                
                print(f"   âœ… Response generated:")
                print(f"      Type: {result.response_type}")
                print(f"      Confidence: {result.confidence_score:.2f}")
                print(f"      Quality: {result.quality_assessment.value}")
                
                # Show response preview
                response_preview = result.response[:200]
                if len(result.response) > 200:
                    response_preview += "..."
                print(f"      Answer: {response_preview}")
                
                # Show citations
                if result.citations:
                    print(f"      Sources: {len(result.citations)} citations available")
                
                # Show follow-ups
                if result.suggested_followups:
                    print(f"      Follow-ups: {result.suggested_followups[0]}")
                
            except Exception as e:
                print(f"   âŒ Error processing question: {e}")
        
        print(f"\nâœ¨ Demo completed! The system can answer various types of research questions.")
        return True
        
    except Exception as e:
        print(f"âŒ Demo failed: {e}")
        return False


def test_interactive_features():
    """Test interactive features and user interface"""
    
    print("\nğŸ® Interactive Features Test")
    print("=" * 40)
    
    try:
        print("ğŸ”§ Testing interactive session setup...")
        
        # Test that we can initialize an interactive session
        processor = QueryProcessor()
        available_dbs = processor.embedder.list_vector_databases()
        
        if available_dbs:
            db_id = available_dbs[0]['database_id']
            print(f"âœ… Interactive session ready with database: {db_id}")
            print(f"   â€¢ Available for real-time Q&A")
            print(f"   â€¢ Supports follow-up questions")
            print(f"   â€¢ Provides confidence scores")
            print(f"   â€¢ Shows source citations")
            
            print(f"\nğŸš€ To start interactive session, run:")
            print(f"   python test_query_processor.py --interactive")
            print(f"   or")
            print(f"   from query_processor import interactive_query_session")
            print(f"   interactive_query_session()")
            
            return True
        else:
            print("âŒ No vector databases available for interactive session")
            return False
            
    except Exception as e:
        print(f"âŒ Interactive features test failed: {e}")
        return False


if __name__ == "__main__":
    print("ğŸ§ª Query Processing & Response Generation Test Suite")
    print("=" * 60)
    
    # Check for interactive mode
    if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
        print("ğŸ® Starting interactive query session...")
        try:
            interactive_query_session()
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Interactive session ended.")
        sys.exit(0)
    
    # Run main test
    main_success = test_query_processor()
    
    # Run demo if main test passes
    if main_success:
        demo_success = demo_question_answering()
        interactive_success = test_interactive_features()
    else:
        print("\nğŸ”„ Running basic component tests...")
        try:
            processor = QueryProcessor(enable_firebase=False, enable_openai=False)
            print("âœ… Query processor can be initialized without external services")
            demo_success = False
            interactive_success = False
        except Exception as e:
            print(f"âŒ Basic initialization failed: {e}")
            demo_success = False
            interactive_success = False
    
    print("\n" + "=" * 60)
    if main_success and demo_success and interactive_success:
        print("ğŸ‰ All tests passed! Query processing pipeline is ready.")
        print("ğŸš€ Your RAG system is now complete and ready for use!")
        print("")
        print("ğŸ“‹ Complete Pipeline Summary:")
        print("   âœ… Step 1: Document Processing (PDF â†’ Text, Tables, Images)")
        print("   âœ… Step 2: Content Analysis (Semantic Chunking + Structure)")
        print("   âœ… Step 3: Vector Embedding (FAISS + Firebase Storage)")
        print("   âœ… Step 4: Query Processing (Q&A + Response Generation)")
        print("")
        print("ğŸ’¡ To start asking questions about your documents:")
        print("   python test_query_processor.py --interactive")
        
    elif main_success:
        print("ğŸ¯ Main query processing functionality working!")
        print("âš ï¸  Some demo features had issues, but core functionality is ready.")
    else:
        print("âš ï¸  Some tests failed. Check logs for details.")
        print("ğŸ’¡ Ensure Firebase is configured and vector database exists.")
        print("ğŸ“‹ Required: Vector database from Step 3 (vector_embedder.py)")
        print("ğŸ”‘ Optional: OPENAI_API_KEY for enhanced response generation") 